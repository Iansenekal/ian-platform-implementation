# AI Platform Deployment Documentation

## 50.50 Monitoring & Logging - Workflow, Transcription, and eSign Metrics

LAN-only • SLO-focused metrics • POPIA-aware telemetry  
Defines what to measure for (1) Approval workflows, (2) Whisper transcription, and (3) eSign/signature verification - including Prometheus metrics, Grafana dashboard views, alerting rules, and evidence-grade audit pointers.

## Document Control

| Field | Value |
|---|---|
| Document ID | `docs/50-Monitoring-Logging/50.50-Workflow-Transcription-eSign-Metrics` |
| Filename (target) | `docs/50-Monitoring-Logging/50.50-Workflow-Transcription-eSign-Metrics.md` |
| Version | `v1.0` |
| Date | `2026-02-10` |
| Owner | Platform Ops + SecOps |
| Status | Operational standard |
| Hard requirements | No sensitive content in metrics; use counts/latency only; actionable alerts; align with audit trail and retention |
| Related docs | 50.10 Scrape plan; 50.20 Dashboards; 71.40 Mail audit events; 60.* Transcription; 70.* eSign |

## 1. Guiding principles for metrics

- Metrics are aggregated and content-free: never export transcript text, document content, names, emails, or token values.
- Prefer service health and performance metrics: throughput, queue depth, latency, error rates.
- Use labels carefully: avoid high-cardinality labels (per document ID) in Prometheus; store detailed per-item traces in audit logs instead.
- Define SLOs: approval time-to-decision, transcription turnaround time, signature verification success rate.

## 2. Metric domains and ownership

| Domain | Primary service | Dashboards | Alerts |
|---|---|---|---|
| Workflow approvals | Workflow engine + Notification router | Approval SLO, Mail Delivery, Escalations | Stalled approvals, high failure rate |
| Transcription | Whisper service + job queue | Throughput, Queue depth, GPU/CPU, Latency | Queue backlog, GPU failure, high error rate |
| eSign/signature verification | Signer/verifier service | Signature success rate, Verification latency | Verification failures, key/cert expiry |

## 3. Workflow (approval chain) metrics

### 3.1 Core metrics (Prometheus style)

| Metric name | Type | Meaning | Notes/Labels |
|---|---|---|---|
| `workflow_approval_requests_total` | counter | Number of approval requests created | label: `doc_type`, `step_role` |
| `workflow_approval_decisions_total` | counter | Approved/rejected outcomes | label: `outcome`, `doc_type` |
| `workflow_approval_time_seconds` | histogram | Time from request->decision | label: `doc_type`, `step_role` |
| `workflow_escalations_total` | counter | Escalations triggered | label: `doc_type` |
| `workflow_pending_approvals` | gauge | Current pending approvals | label: `doc_type` |
| `workflow_failures_total` | counter | Workflow processing failures | label: `stage` |

SLO recommendation (defaults):

- 95% of approvals decided within SLA (client-defined, e.g., 24h).
- Escalation rate < 10% for normal operations.
- Workflow failure rate < 1% of requests.

### 3.2 Email delivery metrics (actionable emails)

| Metric name | Type | Meaning | Notes |
|---|---|---|---|
| `notif_emails_sent_total` | counter | Emails handed to relay | label: `template` |
| `notif_email_delivery_failures_total` | counter | Deferred/bounced delivery outcomes | label: `reason` |
| `notif_action_links_clicked_total` | counter | Action link clicks | label: `action` |
| `notif_action_tokens_issued_total` | counter | Tokens minted (no values) | label: `action` |
| `notif_action_token_replays_total` | counter | Replay blocked events | |

## 4. Transcription (Whisper) metrics

### 4.1 Core transcription pipeline metrics

| Metric name | Type | Meaning | Notes |
|---|---|---|---|
| `transcription_jobs_total` | counter | Jobs submitted | label: `source` (nextcloud/upload) |
| `transcription_jobs_completed_total` | counter | Jobs completed successfully | |
| `transcription_jobs_failed_total` | counter | Jobs failed | label: `reason` |
| `transcription_job_duration_seconds` | histogram | Transcription duration per job | label: `model` |
| `transcription_queue_depth` | gauge | Jobs waiting in queue | |
| `transcription_active_workers` | gauge | Workers currently processing | |

Compute/GPU telemetry signals (as available):

- GPU utilization %, GPU memory %, temperature (exporter dependent).
- CPU load, RAM, disk I/O, and storage fullness for transcript folders.

## 5. eSign and signature verification metrics

| Metric name | Type | Meaning | Notes |
|---|---|---|---|
| `esign_sign_requests_total` | counter | Signature requests initiated | label: `doc_type` |
| `esign_sign_completed_total` | counter | Sign operations completed | |
| `esign_sign_failed_total` | counter | Sign failures | label: `reason` |
| `esign_verify_requests_total` | counter | Verification operations run | |
| `esign_verify_failed_total` | counter | Verification failures | label: `reason` |
| `esign_operation_duration_seconds` | histogram | Time to sign/verify | label: `op` |
| `esign_cert_expiry_days` | gauge | Days until certificate expiry | label: `cert_role` |

## 6. Grafana dashboard views (recommended)

- Workflow SLO dashboard: pending approvals, time-to-decision histograms, escalation rate, failure rate.
- Mail delivery dashboard: sent vs deferred vs bounced, action click-through, replay blocks.
- Transcription dashboard: queue depth, throughput, duration percentiles, failures by reason, worker health.
- eSign dashboard: sign/verify success rate, latency percentiles, cert/key expiry countdown, failure reasons.
- Capacity dashboard: disk usage for `/opt` and Nextcloud data volumes; compute utilization.

## 7. Alerting rules (recommended defaults)

### 7.1 Workflow alerts

- Pending approvals above threshold for > X minutes (queue jam).
- Approval time-to-decision p95 exceeds SLA for Y hours.
- Workflow failure rate exceeds 1% over 15 minutes.

### 7.2 Mail alerts

- Delivery failures spike above baseline (deferred/bounced).
- Action token replays spike (possible phishing/confusion).
- SMTP relay queue depth growing continuously.

### 7.3 Transcription alerts

- Queue depth above threshold for > X minutes.
- Job failure rate exceeds threshold (e.g., > 2% over 15 minutes).
- Worker heartbeat missing / service down.
- Disk fullness for transcript storage > 80% (warning) / > 90% (critical).

### 7.4 eSign alerts

- Signature verification failure rate above threshold.
- Certificate expiry < 30 days (warning) / < 7 days (critical).
- Signing service down or error rate spike.

## 8. Linking metrics to audit trail

- Prometheus metrics provide health and performance signals; they do NOT replace audit logs.
- Use `correlation_id` in application logs/audit events to investigate specific approval or signing incidents.
- Dashboards should provide quick pivots to log search (by time range and component).

## 9. Verification checklist

1. Generate a test approval chain and confirm workflow metrics counters increase appropriately.
2. Trigger a test transcription and confirm queue depth, duration histogram, and completion counters update.
3. Run a test sign+verify and confirm success counters update and latency histograms populate.
4. Force one failure in each domain and confirm alerts trigger and logs provide `correlation_id` for investigation.

## 10. Client rollout variables

| Variable | Client value |
|---|---|
| Approval SLA targets (per doc type) | `<fill>` |
| Escalation thresholds and cadence | `<fill>` |
| Transcription turnaround target (p95) | `<fill>` |
| Alert thresholds (queue depth, error rates) | `<fill>` |
| Cert expiry alert thresholds | `<fill>` |
| Recipient logging mode (count/hash/full) - affects investigation approach | `<fill>` |
