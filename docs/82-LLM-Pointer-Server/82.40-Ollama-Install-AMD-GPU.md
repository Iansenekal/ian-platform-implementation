# 82.40 LLM Pointer Server Ollama Install (AMD GPU) + Service Management + Health Checks

Document ID: `docs/82-LLM-Pointer-Server/82.40-Ollama-Install-AMD-GPU`  
Version: `v1.0`  
Date: `2026-02-10`  
Owner: `AI Infrastructure / Platform Ops`

## Purpose
Install and operate Ollama on `ai-llm01` with deterministic service configuration, LAN-only access model, and health checks.

## Scope
Host-native Ollama install, systemd management, baseline health checks. Model governance and advanced network proxy controls are handled in follow-on runbooks.

## Prerequisites
- `82.20` base OS install complete.
- `82.30` hardening baseline applied.
- AMD GPU checks from `82.10` complete.

## Architecture Baseline
- Ollama runs as a systemd service on Ubuntu 24.04.
- Model store path fixed at `/opt/ollama/models`.
- LAN-only access enforced by UFW allowlist.
- Allowed caller is backend gateway (`hai-backend01`) by default.

## Best-Practice Defaults
- `OLLAMA_HOST` bind policy: use `0.0.0.0:11434` with strict UFW source allowlist.  
  Alternative for higher isolation: `127.0.0.1:11434` behind local TLS proxy.
- ROCm method: follow your proven central AMD RX 7900 XT guide exactly; keep one documented ROCm version per environment and avoid ad-hoc upgrades.

## Repo Implementation
- Inputs: `infrastructure/ollama-gpu/82.40-ollama-install-inputs.env.example`
- Apply script: `infrastructure/ollama-gpu/82.40-ollama-install-apply.sh`
- Verify script: `infrastructure/ollama-gpu/82.40-ollama-install-verify.sh`
- systemd override template: `infrastructure/ollama-gpu/82.40-ollama-override.conf.baseline`
- Evidence checklist: `infrastructure/ollama-gpu/82.40-evidence-checklist.md`

## Execution (on ai-llm01)
1. Copy inputs file and fill values.
2. Run install/apply:
   - `sudo bash infrastructure/ollama-gpu/82.40-ollama-install-apply.sh --env-file infrastructure/ollama-gpu/82.40-ollama-install-inputs.env`
3. Run verify:
   - `bash infrastructure/ollama-gpu/82.40-ollama-install-verify.sh --env-file infrastructure/ollama-gpu/82.40-ollama-install-inputs.env`

## Mandatory Verification
- `ollama --version` succeeds.
- `systemctl status ollama` shows active.
- port `11434` is listening on expected interface.
- local `/api/tags` returns JSON.
- remote `/api/tags` succeeds from allowed caller only.
- UFW rules match allowlist posture.
- AMD GPU device nodes and driver bindings are present.

## Evidence
- `ollama --version`
- `systemctl status ollama --no-pager`
- `ss -tulpen | grep 11434`
- `curl -s http://127.0.0.1:11434/api/tags | jq .`
- remote allowlisted curl output
- `ufw status verbose`
- GPU checks (`lspci -k`, `/dev/dri`, `/dev/kfd`, `rocm-smi` if installed)
