# 82.00 LLM Pointer Server Overview and Role in the Platform

Document ID: `docs/82-LLM-Pointer-Server/82.00-Overview-and-RoleInPlatform`  
Version: `v1.0`  
Date: `2026-02-10`  
Owner: `Platform / AI Infrastructure`

## Purpose
Define the LLM Pointer Server role (dedicated Ollama host), why it is separated from application VMs, and runtime constraints for LAN-only operation plus US/EU model policy.

## Scope
Architecture and operating standards only. Installation and hardening steps are covered in `82.10+`.

## 1) Role Definition
The LLM Pointer Server is a dedicated Ubuntu 24.04 host running Ollama and minimal support services. It exposes a LAN-only inference endpoint to platform services while isolating model runtime/GPU resources from business data stores.

Pointer pattern:
- Application services call a controlled internal inference endpoint.
- Models are not installed independently on every app VM.

## 2) Why It Is Separate
- Security isolation and reduced blast radius.
- Operational stability under GPU-heavy load.
- Centralized model governance and auditability.
- Dedicated GPU tuning without affecting core platform services.
- Controlled model lifecycle updates independent from app deployment cadence.

## 3) Platform Dependencies
Primary:
- `hai-backend01` gateway/services (embeddings/chat).
- Search/knowledge graph enrichment pipelines (policy-controlled).
- Voice post-processing workflows (summarization/tagging).

Secondary/future:
- n8n/Windmill LLM tasks.
- Workflow/eSign assistance where policy allows.

## 4) Runtime Constraints (Hard Requirements)
### 4.1 LAN-only
- No public DNS records.
- No WAN NAT/port-forward into pointer server.
- UFW deny-by-default with explicit allowlist sources.
- Internal TLS preferred for service-to-service HTTP.

### 4.2 US/EU Model Policy
- Only approved US/EU provenance models.
- Controlled model acquisition event:
- approval
- download
- checksum/hash record
- staged deployment
- No auto-update of models without change control.

### 4.3 No Runtime Cloud Dependency
- Inference must work while internet-disconnected.
- Any internet use for initial model pull is documented and time-bounded.
- Production prompts/outputs/transcripts remain on-prem.

## 5) Trust Boundaries and Data Minimization
- Gateway-mediated model calls (UI -> backend gateway -> pointer server).
- Minimize request context sent to model runtime.
- Avoid bulk raw document transfer.
- Log metadata by default (request ID/model/timestamp), redact sensitive content.

## 6) Availability and Failure Modes
- AI-enhanced features degrade gracefully if pointer server unavailable.
- Gateway enforces timeout/circuit-breaker/fallback behavior.
- Missing or unapproved model requests are rejected and audited.
- Slow inference must be bounded via token/time limits.

## 7) Placement and Sizing
Lab baseline:
- Intel i5 11th gen
- 32GB RAM
- AMD RX 7900 XT passthrough

Scale strategy:
- Horizontal (multiple pointer servers behind internal LB) and/or vertical scaling (VRAM/RAM).

## 8) High-level Ports and Sources
| Service | Port | Allowed sources | Notes |
|---|---|---|---|
| Ollama API | `11434/tcp` | backend gateway only (preferred) | Internal TLS proxy recommended |
| SSH admin | `22/tcp` | admin workstation allowlist | key-based auth only |

## 9) Audit and Evidence Requirements
- Model inventory with provenance, version, approval ID, checksums.
- Metadata-level access logs tied to request IDs.
- Change events for model/config/firewall updates.
- Retention aligned with `03.40` and `06.40`.

## 10) Client Rollout Variables
| Variable | Client value |
|---|---|
| Pointer server hostname/IP | `ai-llm01 / 10.10.5.177` |
| Approved caller IPs | `<fill>` |
| Model allowlist owner | `<fill>` |
| Internal TLS method | `<fill>` |
| GPU model/passthrough method | `AMD RX 7900 XT passthrough` |
| Offline mode requirement | `yes/no` |
| Logging level | `metadata-only/limited-content` |

## 11) Next Sequence Docs
- `82.10` Hardware/BIOS/IOMMU and passthrough checks.
- `82.20` Ports/boundaries and UFW policy.
- `82.30` Model governance (allowlist/provenance/checksums).
- `82.40` Ollama install/config.
- `82.50` Healthchecks and watchdog behavior.
- `82.60` Backup/restore for model store and config.
- `82.70` Verification checklist.
