# AI Platform Deployment Documentation

## 05.30 Day-2 Operations - Incident Response (Security + Reliability)

LAN-only runtime • POPIA-friendly • Two-VM on Proxmox • Ubuntu 24.04  
Defines incident severity, containment-first actions, communication, evidence handling, and recovery workflow. (No code.)

## Document Control

| Field | Value |
|---|---|
| Document ID | `docs/05-Operations-Day2/05.30-IncidentResponse` |
| Filename (target) | `docs/05-Operations-Day2/05.30-IncidentResponse.md` |
| Version | `v1.0` |
| Date | `2026-02-09` |
| Owner | Choice IT / Security + Platform Ops |
| Status | Runbook (mandatory) |
| Scope | Incident handling for platform outages, data integrity issues, suspicious access, or compromise |
| Hard requirements | Containment-first; preserve evidence; least privilege; POPIA considerations; post-incident review |
| Related docs | 06.* Backup/DR; 03.* Security/POPIA; 50.* Monitoring; 05.20 Log Review; 04.* Identity/MFA |
| Time zone | Africa/Johannesburg |
| Approver | Platform Owner + Security |
| Note | If compromise is suspected, do not "fix fast". Isolate and preserve evidence first |

## Contents

- 1. Incident goals and principles
- 2. Severity model (P0-P3) and triggers
- 3. Roles and communications
- 4. Containment-first playbook (suspected compromise)
- 5. Reliability incident playbook (outage/performance)
- 6. Data integrity incident playbook
- 7. Recovery and restoration paths
- 8. Evidence pack and reporting
- 9. Post-incident review (PIR) and hardening follow-up
- 10. Client rollout variables (template)

## 1. Incident goals and principles

- Primary goals: (1) protect people and data; (2) contain impact; (3) restore service; (4) learn and prevent recurrence.
- Containment-first: isolate suspected systems before making changes.
- Preserve evidence: logs and timestamps matter for audit and legal requirements.
- Least privilege: only assigned responders access affected systems.
- POPIA: minimize data exposure during investigation; avoid copying personal data unnecessarily.

## 2. Severity model (P0-P3) and triggers

| Priority | Definition | Examples | Target response | Escalation |
|---|---|---|---|---|
| P0 | Security-critical or platform-wide outage | Ransomware suspicion, data exfil, IdP compromise, platform down for all users | Immediate (minutes) | DR Commander + Security + Client Owner |
| P1 | Major degradation or partial outage | Gateway down, Nextcloud down, backups failing for critical data | < 1 hour | Ops lead + Security |
| P2 | Limited impact issue | Single connector failing, intermittent UI errors | Same business day | Ops |
| P3 | Low impact / informational | Minor warnings with no symptoms | Planned review | Ops |

## 3. Roles and communications

- Incident Commander: owns coordination, severity classification, and decision making.
- Platform Operator: executes technical containment and recovery actions.
- Security Officer: leads security investigations, approves secret access/rotation, validates containment.
- Comms Owner: informs stakeholders, maintains timeline updates, ensures consistent messaging.
- Auditor/Compliance: ensures evidence preservation and POPIA policy compliance.

### 3.1 Communication cadence

- P0: updates every 30 minutes until stabilized.
- P1: updates every 60 minutes until stabilized.
- P2: daily updates until resolved.

## 4. Containment-first playbook (suspected compromise)

### 4.1 Immediate actions (first 15 minutes)

1. Classify severity (likely P0) and declare incident; open incident ticket and start timeline log.
2. Isolate affected VM(s): restrict network access to only responder IPs (quarantine).
3. Preserve evidence: do not delete logs; stop log rotation changes; capture current time sync status.
4. Disable risky access: suspend suspect accounts; revoke active sessions/tokens where possible.
5. Freeze changes: stop all upgrades and non-essential operations.

### 4.2 Stabilization actions (next 60 minutes)

6. Identify blast radius: which services, which users, which data domains.
7. Check for persistence: new admin accounts, scheduled jobs, modified configs, new shares/links.
8. Rotate secrets if compromise is credible: IdP admin, DB creds, service keys (document and approve).
9. Decide recovery path: restore-to-new VMs from last known good backup (preferred).
10. Notify compliance if personal data exposure is suspected; follow client escalation rules.

### 4.3 Recovery actions (after containment)

11. Rebuild clean environment if needed (restore-to-new).
12. Restore data and configs from trusted backups predating the incident.
13. Validate security posture: UFW allowlists, SSO/MFA enforcement, RBAC, audit capture.
14. Re-enable user access only after Security Officer signs off.

## 5. Reliability incident playbook (outage/performance)

15. Confirm scope: full outage vs partial vs degraded performance.
16. Check monitoring and recent changes (upgrades, config changes, firewall changes).
17. Check resource pressure: CPU, RAM, disk, IO, and container restart loops.
18. Restart dependents before data stores (see 05.00 Start/Stop).
19. If data stores are unhealthy, plan controlled restart and integrity checks.
20. If issue persists, escalate to restore procedure (06.20) or DR (06.30) based on severity.

## 6. Data integrity incident playbook

21. Freeze writes if possible (disable ingestion/workflows) to prevent further damage.
22. Identify affected dataset(s): Postgres DB, OpenSearch index, Nextcloud files, graph store.
23. Validate integrity using platform-native checks; compare to expected baselines.
24. If corruption confirmed, restore from last known good backup set (06.20).
25. After restore, validate ACL inheritance and permissions to avoid cross-project leakage.

## 7. Recovery and restoration paths

- Minor issue: restart services/config rollback (05.00 + 05.10).
- Major issue: restore procedure (06.20) using verified backup set.
- Disaster: DR runbook (06.30) - restore-to-new VMs, DNS updates, controlled failover.

## 8. Evidence pack and reporting

- Incident timeline log (timestamps + actions + owners).
- Monitoring screenshots/log extracts showing symptoms and recovery.
- Audit log extracts for authentication/admin/share events (P0 categories).
- Backup set IDs used and integrity verification proof (if restore performed).
- Secret rotation record (who approved; who accessed secret backups).
- User impact summary and data exposure assessment (if applicable).

## 9. Post-incident review (PIR) and hardening follow-up

- Hold PIR within 5 business days (P0/P1) or 10 days (P2).
- Document: root cause, contributing factors, detection gaps, and prevention actions.
- Update runbooks, monitoring alerts, and security baselines based on findings.
- Track follow-up tasks to completion and review effectiveness after 30 days.

## 10. Client rollout variables (template)

| Variable | Value |
|---|---|
| Incident Commander (primary/backup) | `<fill>` |
| Security Officer (primary/backup) | `<fill>` |
| Comms owner | `<fill>` |
| Client escalation contacts | `<fill>` |
| Quarantine method (network rules) | `<fill>` |
| Approved responder IPs | `<fill>` |
| Secret rotation approvers | `<fill>` |
| Legal/compliance notification rules | `<fill>` |
| P0/P1 update cadence channel | `<fill>` |
