# AI Platform Deployment Documentation

## 06.30 Backup + DR - Disaster Recovery (DR) Runbook

LAN-only runtime • POPIA-friendly • Two-VM on Proxmox • Ubuntu 24.04  
Defines DR scenarios, decision points, failover steps, and validation/evidence. (No code.)

## Document Control

| Field | Value |
|---|---|
| Document ID | `docs/06-Backup-DR/06.30-DR-Runbook` |
| Filename (target) | `docs/06-Backup-DR/06.30-DR-Runbook.md` |
| Version | `v1.0` |
| Date | `2026-02-09` |
| Owner | Choice IT / Platform Ops + Security |
| Status | Runbook (mandatory) |
| Scope | Disaster Recovery plan for AI Platform: failover decisioning, rebuild paths, restore order, validation |
| Hard requirements | LAN-only; no cloud runtime dependencies; secrets protected; least privilege; evidence required |
| Related docs | 06.00 Scope; 06.10 Schedules; 06.20 Restore; 05.30 Incident Response; 50.40 Monitoring verification |
| Time zone | Africa/Johannesburg |
| Approver | Client IT Owner + Security + Compliance |
| Note | DR is not 'backup'. DR requires rehearsals and documented decision making |
| Lab defaults | AI-DATA01 `10.10.5.178`; AI-FRONTEND01 `10.10.5.179`; DNS `10.10.5.160`; GW `10.10.5.1` |

## Contents

- 1. DR goals and definitions
- 2. DR roles and responsibilities
- 3. DR scenarios covered
- 4. DR decision tree (when to failover)
- 5. DR execution runbooks (step-by-step)
- 6. DR validation checklist
- 7. DR evidence pack
- 8. DR rehearsal schedule
- 9. Client rollout variables (template)

## 1. DR goals and definitions

- Goal: restore business-critical platform capability within agreed RTO, with data loss not exceeding RPO.
- RPO: maximum acceptable data loss window.
- RTO: maximum acceptable time to restore service.
- Failover: switching operations to standby infrastructure.
- Rebuild: re-provisioning the platform from documentation and backups.

## 2. DR roles and responsibilities

| Role | Responsibilities | Primary/Backup |
|---|---|---|
| DR Commander | Declares disaster, approves failover path, coordinates stakeholders | `<fill> / <fill>` |
| Platform Operator | Executes restore and service bring-up steps | `<fill> / <fill>` |
| Security Officer | Approves secret handling, validates access controls, monitors for abuse | `<fill> / <fill>` |
| Compliance Officer | Ensures POPIA retention/legal hold requirements, signs evidence pack | `<fill> / <fill>` |
| Network/DNS Admin | Updates internal DNS/records, verifies routing, firewall allowlists | `<fill> / <fill>` |

## 3. DR scenarios covered

- Scenario A: AI-DATA01 VM failure (disk corruption or VM lost)
- Scenario B: AI-FRONTEND01 VM failure
- Scenario C: Proxmox host failure (single host)
- Scenario D: Primary storage failure (backup destination still available)
- Scenario E: Ransomware/compromise suspected (containment-first DR)
- Scenario F: Total site outage (power/network) - if a secondary site exists

## 4. DR decision tree (when to failover)

1. Confirm incident severity and containment status (security first).
2. Determine if this is a restore-in-place (minor) or DR declaration (major).
3. If compromise suspected: isolate first, preserve evidence, rotate secrets, then restore to clean infrastructure.
4. Choose recovery path: restore-to-new VMs (preferred) vs in-place restore (only if safe).
5. Confirm backup set integrity and last known good restore point.

## 5. DR execution runbooks (step-by-step)

### 5.1 DR Runbook A - AI-DATA01 VM lost (restore-to-new VM)

6. Declare DR and open a DR ticket; record start time and approval.
7. Provision a new Ubuntu 24.04 VM on Proxmox with the approved sizing and static IP (replacement for `10.10.5.178`).
8. Apply baseline hardening: SSH restrictions, UFW default deny, time sync, and required admin accounts.
9. Restore platform configuration directories (`/opt/<stack>/`), systemd units, and firewall policy assets from backup.
10. Restore secrets from encrypted backup under dual-control (if required).
11. Restore Postgres first, then OpenSearch, then Nextcloud DB/data (if enabled), then automation DBs (if enabled).
12. Start services in dependency order: core data -> IdP -> Gateway -> Search/Graph -> Nextcloud -> automation -> monitoring.
13. Update internal DNS if hostnames change; verify AI-FRONTEND01 can reach Gateway via allowed endpoints only.
14. Run full security validation (ports, UFW, SSO/MFA, RBAC) before opening to users.
15. Run functional smoke tests and monitoring verification; record end time and recovery summary.

### 5.2 DR Runbook B - AI-FRONTEND01 VM lost (restore-to-new VM)

16. Declare partial DR if needed; record approvals.
17. Provision a new Ubuntu 24.04 VM for AI-FRONTEND01 (replacement for `10.10.5.179`).
18. Apply baseline hardening and UFW allowlists for LAN-only UI access.
19. Restore `/opt/<stack>/` config for frontend, internal TLS assets, and systemd units from backup.
20. Validate SSO client settings and UI SSO redirect URLs (internal).
21. Confirm connectivity: Frontend can talk ONLY to Gateway and approved internal services.
22. Run UI login and functional smoke tests; validate monitoring and dashboards visibility.

### 5.3 DR Runbook C - Suspected compromise (containment-first DR)

23. Immediately isolate affected VM(s) from the LAN (network quarantine) while preserving forensic evidence.
24. Do NOT perform in-place restore. Provision clean replacement VMs (restore-to-new).
25. Rotate secrets and credentials (IdP admin, DB creds, service keys) before re-opening services.
26. Restore from last known good backup set predating compromise; validate audit logs for scope.
27. Perform post-restore hardening validation and security review before resuming operations.

### 5.4 DR Runbook D - Proxmox host failure (single host)

28. Confirm whether backups are stored off-host (NAS/backup server).
29. Bring up replacement Proxmox host or restore VMs to alternate host.
30. Restore VM backups and then execute Runbooks A and B as applicable.

### 5.5 DR Runbook E - Total site outage (if secondary site exists)

31. Confirm secondary site connectivity and capacity.
32. Bring up standby VMs at secondary site and restore from replicated/offsite backups.
33. Update internal DNS and routing for users at the secondary site.
34. Operate in DR mode until primary returns; then plan controlled failback.

## 6. DR validation checklist

- Security: UFW allowlists applied; no unexpected ports open; SSH hardening intact.
- Identity: SSO works; MFA enforced for admins; RBAC group mapping correct; break-glass accounts present.
- Data: Postgres integrity validated; OpenSearch health green/yellow acceptable; Nextcloud consistency (if enabled).
- Function: UI login works; Gateway health endpoints OK; Search respects ACLs; automation test job passes (if enabled).
- Observability: monitoring targets UP; audit events captured; alerting tested.

## 7. DR evidence pack

- DR declaration approval and timeline (start/end timestamps).
- Backup set ID used and integrity verification proof.
- Security validation evidence (ports/UFW/SSO/MFA/RBAC).
- Functional smoke test evidence and monitoring verification evidence.
- Secret handling record (who accessed secret backups; dual-control sign-off).
- Post-incident review summary and follow-up actions.

## 8. DR rehearsal schedule

- Quarterly: tabletop DR exercise (scenario walkthrough).
- Bi-annual: technical restore-to-new VM rehearsal in lab (Runbook A + B).
- After major architecture changes: rerun rehearsal within 30 days.
- Record outcomes, gaps, and updates to this runbook.

## 9. Client rollout variables (template)

| Variable | Value |
|---|---|
| Target RPO (hours) | `<fill>` |
| Target RTO (hours) | `<fill>` |
| Backup destination location | `<fill>` |
| Secondary site exists (Yes/No) | `<fill>` |
| Secondary site IP ranges / DNS changes | `<fill>` |
| DR Commander name | `<fill>` |
| Primary platform operator | `<fill>` |
| Security officer | `<fill>` |
| Compliance officer | `<fill>` |
| DNS admin | `<fill>` |
| Preferred recovery mode (restore-to-new / in-place) | `<fill>` |
| Last known good restore point policy | `<fill>` |
